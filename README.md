# OpenAI Guardrails

> **Choose your language:**
> - [Python](#python)
> - [TypeScript](#typescript)

---

# Python

## Overview

OpenAI Guardrails is a Python package for adding robust, configurable safety and compliance guardrails to LLM applications. It provides a drop-in wrapper for OpenAI's Python client, enabling automatic input/output validation and moderation using a wide range of guardrails.

## Documentation

For full details, advanced usage, and API reference, see here: [OpenAI Guardrails Alpha Documentation](https://oaig-whisper-yonder-xnjpy2.vercel.app/docs/).

## Quick Start: Using OpenAI Guardrails (Python)

1. **Generate your guardrail spec JSON**
   - Use the [Guardrails web UI](https://oaig-whisper-yonder-xnjpy2.vercel.app/) (pw: guardrails) to create a JSON configuration file describing which guardrails to apply and how to configure them.
   - The wizard outputs a file like `guardrail_specs.json`.

2. **Install dependencies**
   - **Install from this repo:**
     ```bash
     pip install -e .[presidio]
     ```
   - **Eventually this will be:**
     ```bash
     pip install openai-guardrails
     ```

3. **Wrap your OpenAI client with Guardrails**
   ```python
   from guardrails import GuardrailsOpenAI, GuardrailTripwireTriggered
   from pathlib import Path

   # guardrail_config.json is generated by the configuration wizard
   client = GuardrailsOpenAI(config=Path("guardrail_config.json"))

   # Use as you would the OpenAI client, but handle guardrail exceptions
   try:
      response = client.chat.completions.create(
          model="gpt-5",
          messages=[{"role": "user", "content": "..."}],
      )
      print(response.llm_response.choices[0].message.content)
   except GuardrailTripwireTriggered as e:
      # Handle blocked or flagged content
      print(f"Guardrail triggered: {e}")
   # ---
   # Example: Using the new OpenAI Responses API with Guardrails
   try:
      resp = client.responses.create(
          model="gpt-5",
          input="What are the main features of your premium plan?",
          # Optionally, add file_search or other tool arguments as needed
      )
      print(resp.llm_response.output_text)
   except GuardrailTripwireTriggered as e:
      print(f"Guardrail triggered (responses API): {e}")
   ```
   - The client will automatically apply all configured guardrails to inputs and outputs.
   - If a guardrail is triggered, a `GuardrailTripwireTriggered` exception will be raised. You should handle this exception to gracefully manage blocked or flagged content.

> **Note:** The Guardrails web UI (in `frontend/`) is hosted [here](https://oaig-whisper-yonder-xnjpy2.vercel.app/). You do not need to run the web UI yourself to use the Python package.

---

## What Does the Python Package Provide?

- **GuardrailsOpenAI** and **GuardrailsAsyncOpenAI**: Drop-in replacements for OpenAI's `OpenAI` and `AsyncOpenAI` clients, with automatic guardrail enforcement.
- **GuardrailsAzureOpenAI** and **GuardrailsAsyncAzureOpenAI**: Drop-in replacements for Azure OpenAI clients, with the same guardrail support. (See the documentation for details.)
- **Automatic input/output validation**: Guardrails are applied to all relevant API calls (e.g., `chat.completions.create`, `responses.create`, etc.).
- **Configurable guardrails**: Choose which checks to enable, and customize their parameters via the JSON spec.
- **Tripwire support**: Optionally block or mask unsafe content, or just log/flag it for review.

---

# TypeScript

## Overview

Guardrails TypeScript is a Node.js/TypeScript framework for secure AI calls with OpenAI Guardrails. It provides a port of the Python Guardrails framework with enhanced type safety, Node.js integration, and a high-level client API mirroring the Python version.

> **Note:** The TypeScript package is currently in development and not yet published to npm. Use the local installation methods below.

## Quick Start: Using OpenAI Guardrails (TypeScript)

### 1. Install Locally

Clone the repository and install dependencies:
```bash
# From the root of the repo
cd guardrails-ts
npm install
npm run build
```

You can also install it into your own project from the local path:
```bash
npm install /absolute/path/to/guardrails/guardrails-ts
```

### 2. Using the Guardrails Client in TypeScript

The TypeScript client now provides a high-level API similar to Python:

```typescript
import { GuardrailsOpenAI } from '@guardrails/guardrails-ts';
import * as path from 'path';
// Create a GuardrailsOpenAI client with your config
const client = await GuardrailsOpenAI.create(path.resolve('guardrails_config.json'), {
  apiKey: process.env.OPENAI_API_KEY,
});
try {
  const response = await client.chat.completions.create({
    model: 'gpt-5',
    messages: [{ role: 'user', content: '...' }],
  });
  console.log(response.llm_response.choices[0].message.content);
} catch (e) {
  // Handle blocked or flagged content
  console.error('Guardrail triggered:', e);
}
// Example: Using the new OpenAI Responses API with Guardrails
try {
  const resp = await client.responses.create({
    model: 'gpt-5',
    input: '...',
    // Optionally, add file_search or other tool arguments as needed
  });
  console.log(resp.llm_response.output_text);
} catch (e) {
  console.error('Guardrail triggered (responses API):', e);
}
```

### 3. CLI Usage

You can also use the CLI for validation and evaluation:

```bash
# Run directly with npm (from guardrails-ts directory)
npm run cli -- --help
npm run cli -- validate config.json
npm run cli -- eval --config-path config.json --dataset-path dataset.jsonl

# Or install globally for CLI access
npm install -g .
guardrails-ts --help
guardrails-ts validate config.json
guardrails-ts eval --config-path config.json --dataset-path dataset.jsonl
```

### 4. Running Examples

```bash
# Build the package first
npm run build

# Run example scripts (from guardrails-ts/examples)
cd examples
npx tsx simple-runtime-usage.ts
npx tsx guardrails-demo.ts
```

---

## What Does the TypeScript Package Provide?

- **GuardrailsOpenAI** and **GuardrailsAzureOpenAI**: Drop-in replacements for OpenAI's `OpenAI` and `AzureOpenAI` clients, with automatic guardrail enforcement (mirrors Python API).
- **Automatic input/output validation**: Guardrails are applied to all relevant API calls (e.g., `chat.completions.create`, `responses.create`, etc.).
- **Configurable guardrails**: Choose which checks to enable, and customize their parameters via the JSON spec.
- **Tripwire support**: Optionally block or mask unsafe content, or just log/flag it for review.
- **CLI tool**: Validate configs, run evaluations, and more from the command line.
- **Evaluation framework**: Test guardrail performance on datasets and measure metrics like precision, recall, and F1 scores.

---

## Available Guardrails

Below is a list of all built-in guardrails you can configure. Each can be enabled/disabled and customized in your JSON spec.

| Guardrail Name           | Description |
|-------------------------|-------------|
| **Keyword Filter**      | Triggers when any keyword appears in text. |
| **Competitors**         | Checks if the model output mentions any competitors from the provided list. |
| **Jailbreak**           | Detects attempts to jailbreak or bypass AI safety measures using techniques such as prompt injection, role-playing requests, system prompt overrides, or social engineering. |
| **Moderation**          | Flags text containing disallowed content categories (e.g., hate, violence, sexual, etc.) using OpenAI's moderation API. |
| **NSFW Text**           | Detects NSFW (Not Safe For Work) content in text, including sexual content, hate speech, violence, profanity, illegal activities, and other inappropriate material. |
| **Contains PII**        | Checks that the text does not contain personally identifiable information (PII) such as SSNs, phone numbers, credit card numbers, etc., based on configured entity types. |
| **Secret Keys**         | Checks that the text does not contain potential API keys, secrets, or other credentials. |
| **Off Topic Prompts**   | Checks that the content stays within the defined business scope. |
| **URL Filter**          | Flags URLs in the text unless they match entries in the allow list. |
| **Custom Prompt Check** | Runs a user-defined guardrail based on a custom system prompt. Allows for flexible content moderation based on specific requirements. |
| **Anti-Hallucination**  | Detects potential hallucinations in AI-generated text using OpenAI Responses API with file search. Validates claims against actual documents and flags factually incorrect, unsupported, or potentially fabricated information. |

---

## License

For the duration of this early access alpha, `guardrails` (including both the Python and TypeScript packages) is distributed under the Alpha Evaluation Agreement that your organization signed with OpenAI.

Both the Python and TypeScript packages are intended to be MIT-licensed in the future, subject to change.

## Disclaimers

Please note that Guardrails may use Third-Party Services such as the [Presidio open-source framework](https://github.com/microsoft/presidio), which are subject to their own terms and conditions and are not developed or verified by OpenAI.

Developers are responsible for implementing appropriate safeguards to prevent storage or misuse of sensitive or prohibited content (including but not limited to personal data, child sexual abuse material, or other illegal content). OpenAI disclaims liability for any logging or retention of such content by developers. Developers must ensure their systems comply with all applicable data protection and content safety laws, and should avoid persisting any blocked content generated or intercepted by Guardrails.